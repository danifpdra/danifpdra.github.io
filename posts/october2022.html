<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title>Daniela's PhD</title>
    <link rel="icon" type="image/x-icon" href="../assets/favicon.ico"/>
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800"
          rel="stylesheet" type="text/css"/>
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="../css/styles.css" rel="stylesheet"/>

    <style>
        * {
            box-sizing: border-box;
        }

        .img-zoom-container {
            position: relative;
        }

        .img-zoom-lens {
            position: absolute;
            border: 1px solid #d4d4d4;
            /*set the size of the lens:*/
            width: 80px;
            height: 80px;
        }

        .img-zoom-result {
            border: 1px solid #d4d4d4;
            /*set the size of the result div:*/
            width: 500px;
            height: 500px;
            margin-left: auto;
            margin-right: auto;
        }


    </style>
    <script>
        function imageZoom(imgID, resultID) {
            var img, lens, result, cx, cy;
            img = document.getElementById(imgID);
            result = document.getElementById(resultID);
            /*create lens:*/
            lens = document.createElement("DIV");
            lens.setAttribute("class", "img-zoom-lens");
            /*insert lens:*/
            img.parentElement.insertBefore(lens, img);
            /*calculate the ratio between result DIV and lens:*/
            cx = result.offsetWidth / lens.offsetWidth;
            cy = result.offsetHeight / lens.offsetHeight;
            /*set background properties for the result DIV:*/
            result.style.backgroundImage = "url('" + img.src + "')";
            result.style.backgroundSize = (img.width * cx) + "px " + (img.height * cy) + "px";
            /*execute a function when someone moves the cursor over the image, or the lens:*/
            lens.addEventListener("mousemove", moveLens);
            img.addEventListener("mousemove", moveLens);
            /*and also for touch screens:*/
            lens.addEventListener("touchmove", moveLens);
            img.addEventListener("touchmove", moveLens);

            function moveLens(e) {
                var pos, x, y;
                /*prevent any other actions that may occur when moving over the image:*/
                e.preventDefault();
                /*get the cursor's x and y positions:*/
                pos = getCursorPos(e);
                /*calculate the position of the lens:*/
                x = pos.x - (lens.offsetWidth / 2);
                y = pos.y - (lens.offsetHeight / 2);
                /*prevent the lens from being positioned outside the image:*/
                if (x > img.width - lens.offsetWidth) {
                    x = img.width - lens.offsetWidth;
                }
                if (x < 0) {
                    x = 0;
                }
                if (y > img.height - lens.offsetHeight) {
                    y = img.height - lens.offsetHeight;
                }
                if (y < 0) {
                    y = 0;
                }
                /*set the position of the lens:*/
                lens.style.left = x + "px";
                lens.style.top = y + "px";
                /*display what the lens "sees":*/
                result.style.backgroundPosition = "-" + (x * cx) + "px -" + (y * cy) + "px";
            }

            function getCursorPos(e) {
                var a, x = 0, y = 0;
                e = e || window.event;
                /*get the x and y positions of the image:*/
                a = img.getBoundingClientRect();
                /*calculate the cursor's x and y coordinates, relative to the image:*/
                x = e.pageX - a.left;
                y = e.pageY - a.top;
                /*consider any page scrolling:*/
                x = x - window.pageXOffset;
                y = y - window.pageYOffset;
                return {x: x, y: y};
            }
        }
    </script>
</head>
<body>
<!-- Navigation-->
<nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
    <div class="container px-4 px-lg-5">
        <a class="navbar-brand" href="../index.html">Daniela's PhD</a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            Menu
            <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ms-auto py-4 py-lg-0">
                <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="../index.html">Home</a></li>
                <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="about.html">About</a></li>
                <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="post.html">Sample Post</a></li>
                <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="contact.html">Contact</a></li>
            </ul>
        </div>
    </div>
</nav>
<!-- Page Header-->
<header class="masthead" style="background-image: url('../assets/img/larcc_2.jpeg')">
    <div class="container position-relative px-4 px-lg-5">
        <div class="row gx-4 gx-lg-5 justify-content-center">
            <div class="col-md-10 col-lg-8 col-xl-7">
                <div class="post-heading">
                    <h1>October 2022 </h1>
                    <h2 class="subheading">Year 3</h2>
                    <span class="meta">
                                Posted by
                                <a href="#!">Daniela Rato</a>
                                on October 24, 2022
                            </span>
                </div>
            </div>
        </div>
    </div>
</header>
<!-- Post Content-->
<article class="mb-4">
    <div class="container px-4 px-lg-5">
        <div class="row gx-4 gx-lg-5 justify-content-center">
            <div class="col-md-10 col-lg-8 col-xl-7">
                <h2 class="section-heading"> Human Pose Estimation - Review on State-of-the-art Methods and
                    Datatests</h2>
                <p></p>

                <p>Human pose estimation (HPE) is the estimation of the configuration of human body parts from data
                    captured by
                    sensors. It provides geometric and motion information of the body and can be used in a large variety
                    of
                    applications such as autonomous driving, video surveillance, robotics, etc. </p>

                <p>There are 3 different types of human body models that are used for human pose estimation. These
                    models
                    are represented in the following image.</p>

                <p><a href="#!"><img class="img-fluid" width="560"
                                     src="../assets/img/human-body-models-pose-estimation.png"
                                     alt="..."/></a></p>

                <h4> HPE approaches </h4>

                <p>Human pose estimation can be divided into two main categories: 2D and 3D. 2D human pose estimation is
                    when the pose of the humans is estimated in a two-dimensional image. 2D HPE can be single or multi
                    person. Single person 2D HPE methods are usually regression methods, where it is applied an
                    end-to-end framework to learn a mapping from the input image to body joints or parameters of human
                    body models; and heatmap based methods, where locations of human body parts and joints are
                    predicted, supervised by heatmap representation. Multi-person 2D HPE is divided in top down and
                    bottom up approaches. Top-down methods employ off-the-shelf person detectors to
                    obtain a set of boxes (each corresponding to one person) from the input images, and then apply
                    single-person pose estimators to each person box to generate multi-person poses. Different from
                    top-down methods, bottom-up methods locate all the body joints in one image first and then group
                    them to the corresponding subjects. In the top-down pipeline, the number of people in the input
                    image will directly affect the computing time. The computing speed for bottom-up methods is
                    usually faster than top-down methods since they do not need to detect the pose for each person
                    separately.
                </p>

                <p>3D human pose estimation methods are more complex, and can be grouped in single view and single
                    person; single view and multi person; multi person and multi view. Single view and person methods
                    can be divided in human mesh recovery and skeleton only methods. The later can also be divided in
                    direct estimation, where the 3D skeleton is retrieved directly from a 2D image, or 2D to 3D lifting,
                    where the skeleton is detected in 2D, and then extrapolated to 3D. Similarly to 2D methods, single
                    view and multi person 3D HPE can be grouped in top down and bottom up approaches. Multi view HPE is
                    usually a combination of the previous methods. </p>

                <p>Some of the more interesting existing methods are <a href="https://github.com/MVIG-SJTU/AlphaPose">
                    AlphaPose </a>, <a href="https://github.com/facebookresearch/DensePose"> DensePose</a>, <a
                        href="https://github.com/ViTAE-Transformer/ViTPose">ViTPose</a>,
                    <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a>, <a
                            href="https://github.com/leijue222/Intra-and-Inter-Human-Relation-Network-for-MPEE">
                        I2R-Net</a> and <a href="https://arxiv.org/abs/2208.11251"> Learnable human mesh triangulation
                        for 3D human pose and shape estimation</a>.</p>

                The main datasets with 3D labels are <a
                    href="http://vision.imar.ro/human3.6m/description.php"><u>Human3.6M</u></a> and <a
                    href="https://vcai.mpi-inf.mpg.de/3dhp-dataset/"><u>MPI-INF-3DHP</u></a>.

                <p></p>

                <h4>Synthetic Data</h4>

                <p> The use of synthetic data to train models has been becoming more popular and reliable among
                    learning. In the HPE field, there are also methodologies that take advantage of the synthetic data
                    benefits, such as ease of acquisition and labelling. Example of this are <a
                            href="https://arxiv.org/abs/1907.02499">Sim2real transfer learning for 3D human pose
                        estimation: motion to the rescue
                    </a>, <a href="https://arxiv.org/abs/1912.04070">Synthetic Humans for Action Recognition from Unseen
                        Viewpoints
                    </a>, <a href="https://arxiv.org/abs/1907.05193">Cross-Domain Complementary Learning Using Pose for
                        Multi-Person Part Segmentation</a>.
                </p>

                <p>There are also synthetic human pose estimation datasets. The following are to be highlighted: <a
                        href="https://www.di.ens.fr/willow/research/surreal/data/"> SURREAL Dataset</a> and <a
                        href="https://github.com/ZheC/GTA-IM-Dataset"> GTA-IM Dataset</a>.</p>

                <h4>RGB-D HPE</h4>

                <p>RGB-D HPE is not a very common study case among the HPE methods. Some authors explore its
                    possibilities,
                    for example <a href="https://arxiv.org/abs/2008.00158"> TexMesh: Reconstructing Detailed Human
                        Texture
                        and Geometry from RGB-D Video</a>, <a href="https://arxiv.org/abs/1908.09999">A2J:
                        Anchor-to-Joint
                        Regression Network for 3D Articulated Pose Estimation from a Single Depth Image</a>, <a
                            href="https://arxiv.org/abs/1804.06023">DoubleFusion: Real-time Capture of Human
                        Performances with
                        Inner Body Shapes from a Single Depth Sensor</a> and <a href="https://arxiv.org/abs/1701.07372">Multi-view
                        RGB-D Approach for Human Pose Estimation in Operating Rooms</a>.</p>

                <p>There are also several datasets that include the depth modalitity and 3D labels, such as <a
                        href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</a>, <a
                        href="https://vcai.mpi-inf.mpg.de/projects/SingleShotMultiPerson/">MuCo-3DHP</a>, <a
                        href="https://sizhean.github.io/mri">mRI</a> and <a
                        href="https://rose1.ntu.edu.sg/dataset/actionRecognition/">NTU RGB+D and NTU RGB+D 120</a></p>


                <h4>Point Cloud Feature Extraction</h4>

                <p>
                    The goal of point cloud feature extraction is to build a segmentation map using a point cloud as an
                    input. This work started with
                    <a href="https://arxiv.org/abs/1612.00593">PointNet</a>, where the methodology was able to find the
                    different feature in a space without great detail. This method was later improved by
                    <a href="https://arxiv.org/abs/1706.02413">PointNet++</a>, which was able to differentiate more
                    features in a tridimensional map. In 2022, the previous was again revised and
                    <a href="https://arxiv.org/abs/2206.04670">PointNeXt</a> was created. This method is able to detect
                    fine details with great precision.
                </p>

                <h4>Current HPE Challenges</h4>

                <p>There are a lot of work already developed in HPE but challenges still remain and are transversal to
                    both 2D and 3D HPE. The main current challenges are:</p>
                <ul>
                    <li>Reliable detection of individual under occlusions</li>
                    <li>Computation efficiency, mainly for cases that require real-time HPE and specially in 3D HPE
                        (more demanding that 2d)
                    </li>
                    <li>Limited data for rare poses and model generalization for in the wild data</li>
                </ul>


                <h4>Experimenting with DensePose</h4>

                <div class="row">
                    <div class="column">
                        <img src="../assets/img/family.jpg" alt="Snow" style="width:100%">
                    </div>
                    <div class="column">
                        <img src="../assets/img/family_densepose_contour_segm_dl.0001.png" alt="Forest" style="width:100%">
                    </div>
                </div>

                <div class="row">
                    <div class="column">
                        <img src="../assets/img/image.jpg" alt="Snow" style="width:100%">
                    </div>
                    <div class="column">
                        <img src="../assets/img/image_densepose_contour_segm_dl.0001.png" alt="Forest" style="width:100%">
                    </div>
                </div>

                <div class="row">
                    <div class="column">
                        <img src="../assets/img/meeting.jpg" alt="Snow" style="width:100%">
                    </div>
                    <div class="column">
                        <img src="../assets/img/meeting_densepose_contour_segm_dl.0001.png" alt="Forest" style="width:100%">
                    </div>
                </div>

                <p></p>

                <!--                <h4>Other tasks</h4>-->
                <!--                <ul>-->
                <!--                    <li>Review and submission of JMS article - first review</li>-->
                <!--                    <li>Development of workplan for the 3 months in Barcelona</li>-->
                <!--                    <li>Organization of new manuscript for depth calibration</li>-->
                <!--                    <li>Writing the introduction</li>-->
                <!--                    <li>State-of-the-art about RGB-D and hand-eye systems</li>-->
                <!--                </ul>-->

                <!--                <h4>Issues </h4>-->
                <!--                &lt;!&ndash;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&ndash;&gt;-->
                <!--                <p><a href="https://github.com/lardemua/larcc/issues/48">Hand eye dataset capturing - -->
                <!--                    open </a>-->
                <!--                </p>-->
                <!--                <p><a href="https://github.com/lardemua/atom/issues/524">Hand-eye calibration not going as expected-->
                <!--                    - open </a>-->
                <!--                </p>-->
                <!--                <p><a href="https://github.com/lardemua/larcc/issues/501">Possible depth labelling problem with robot - -->
                <!--                    open</a>-->
                <!--                </p>-->


                <!--                https://youtube.com/shorts/3dKY3RwgptY?feature=share-->
                <!--                <div class="my-4">-->

                <!--                <h4>Issues </h4>-->
                <!--                &lt;!&ndash;%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&ndash;&gt;-->
                <!--                <p><a href="https://github.com/lardemua/atom/issues/323">Add depth component to ATOM's framework-->
                <!--                    - -->
                <!--                    open </a>-->
                <!--                </p>-->


                <!--                </div>-->
            </div>
        </div>
    </div>
</article>
<!-- Footer-->
<footer class="border-top">
    <div class="container px-4 px-lg-5">
        <div class="row gx-4 gx-lg-5 justify-content-center">
            <div class="col-md-10 col-lg-8 col-xl-7">
                <ul class="list-inline text-center">
                    <li class="list-inline-item">
                        <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                                    </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="#!">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                        </a>
                    </li>
                </ul>
                <div class="small text-center text-muted fst-italic">Copyright &copy; Your Website 2021</div>
            </div>
        </div>
    </div>
</footer>
<!-- Bootstrap core JS-->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/js/bootstrap.bundle.min.js"></script>
<!-- Core theme JS-->
<script src="../js/scripts.js"></script>
</body>
</html>
